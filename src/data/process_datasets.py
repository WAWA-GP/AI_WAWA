import pandas as pd
import numpy as np
import json
from pathlib import Path
import re

print("ğŸš€ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹œì‘!\n")

# 1. ì–´íœ˜ ë°ì´í„° (ì´ë¯¸ ì„±ê³µ)
print("ğŸ“š ì–´íœ˜ ë°ì´í„°ëŠ” ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œ")

# 2. ëŒ€í™” ë°ì´í„° - ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
print("\nğŸ’¬ ëŒ€í™” ë°ì´í„° ì²˜ë¦¬ ì¤‘ (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)...")

conv_file = Path("data/raw/conversations/31. Chat(1.7k).csv")
print(f"íŒŒì¼ ê²½ë¡œ: {conv_file}")

# ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
encodings = ['iso-8859-1', 'latin1', 'cp1252', 'utf-8']
df = None

for encoding in encodings:
    try:
        print(f"ì¸ì½”ë”© ì‹œë„: {encoding}")
        df = pd.read_csv(conv_file, encoding=encoding)
        print(f"âœ… {encoding} ì¸ì½”ë”©ìœ¼ë¡œ ì„±ê³µ!")
        break
    except Exception as e:
        print(f"âŒ {encoding} ì‹¤íŒ¨")
        continue

if df is not None:
    print(f"ì›ë³¸ ë°ì´í„° shape: {df.shape}")
    print(f"ì»¬ëŸ¼ëª…: {df.columns.tolist()}")
    
    # í…ìŠ¤íŠ¸ ì •ì œ
    df['text'] = df['text'].astype(str).str.strip()
    df = df[df['text'] != 'nan']
    df = df[df['text'] != '']
    df = df[df['text'] != 'text']  # í—¤ë” ì¤‘ë³µ ì œê±°
    
    # í†µê³„ ê³„ì‚°
    df['text_length'] = df['text'].str.len()
    df['word_count'] = df['text'].str.split().str.len()
    
    print(f"ì •ì œ í›„ ëŒ€í™” ìˆ˜: {len(df)}")
    print(f"í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {df['text_length'].mean():.1f} ê¸€ì")
    
    # ìƒ˜í”Œ í™•ì¸
    print("\nìƒ˜í”Œ ëŒ€í™” ë°ì´í„° (ì²« 3ê°œ):")
    for i in range(min(3, len(df))):
        text = df.iloc[i]['text']
        preview = text[:100] + "..." if len(text) > 100 else text
        print(f"  {i+1}. {preview}")
    
    # ëŒ€í™” ìŒ ìƒì„±
    conversation_pairs = []
    for i in range(0, len(df)-1, 2):
        if i+1 < len(df):
            conversation_pairs.append({
                'user_input': df.iloc[i]['text'],
                'ai_response': df.iloc[i+1]['text']
            })
    
    pairs_df = pd.DataFrame(conversation_pairs)
    
    # ì €ì¥
    Path("data/processed").mkdir(exist_ok=True)
    pairs_df.to_csv("data/processed/conversation_pairs.csv", index=False, encoding='utf-8')
    
    print(f"âœ… ìƒì„±ëœ ëŒ€í™” ìŒ: {len(pairs_df)}ê°œ")
else:
    print("âŒ ëŒ€í™” ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")

# 3. ë¬¸ì¥ ë°ì´í„° ì²˜ë¦¬
print("\nğŸ“ ì‚¬ì‹¤ ë¬¸ì¥ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

factual_file = Path("data/raw/factual/sentence.txt")

if factual_file.exists():
    print(f"íŒŒì¼ ê²½ë¡œ: {factual_file}")
    
    sentences_data = []
    with open(factual_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            parts = line.strip().split('\t')
            if len(parts) >= 3:
                sentence = parts[0].strip()
                sentences_data.append({
                    'sentence': sentence,
                    'grammar_label': int(parts[1]),
                    'factual_label': int(parts[2]),
                    'word_count': len(sentence.split()),
                    'has_date': bool(re.search(r'\d{4}', sentence)),
                    'sentence_length': len(sentence)
                })
    
    factual_df = pd.DataFrame(sentences_data)
    
    print(f"ì „ì²´ ë¬¸ì¥ ìˆ˜: {len(factual_df)}")
    print(f"ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë¬¸ì¥: {(factual_df['grammar_label'] == 1).sum()}ê°œ")
    
    # ìƒ˜í”Œ ë¬¸ì¥
    print("\nìƒ˜í”Œ ë¬¸ì¥ë“¤:")
    for i in range(min(3, len(factual_df))):
        print(f"  {i+1}. {factual_df.iloc[i]['sentence']}")
    
    # ë¬¸ë²• ì²´í¬ìš© ë°ì´í„° ìƒì„±
    grammar_data = []
    for _, row in factual_df.iterrows():
        grammar_data.append({
            'text': row['sentence'],
            'is_correct': row['grammar_label'] == 1,
            'word_count': row['word_count'],
            'has_date': row['has_date']
        })
    
    grammar_df = pd.DataFrame(grammar_data)
    grammar_df.to_csv("data/processed/grammar_sentences.csv", index=False)
    
    print(f"âœ… ë¬¸ë²• ë°ì´í„° ì €ì¥ ì™„ë£Œ")
else:
    print("âŒ sentence.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

print("\nğŸ¯ ì „ì²˜ë¦¬ ì™„ë£Œ!")
print("ğŸ“ data/processed/ í´ë” í™•ì¸:")
