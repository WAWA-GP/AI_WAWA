"""
AI ì–¸ì–´í•™ìŠµ ì•± ê¸°ëŠ¥ë³„ ì„±ëŠ¥ ì ìˆ˜(%) ì¸¡ì • ì‹œìŠ¤í…œ
ê° ê¸°ëŠ¥ì˜ ì„±ëŠ¥ì„ 0-100% ì ìˆ˜ë¡œ í™˜ì‚°í•˜ì—¬ í‰ê°€
"""

import asyncio
import aiohttp
import time
import statistics
import psutil
import os
import json
import numpy as np
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Tuple

@dataclass
class PerformanceScore:
    """ì„±ëŠ¥ ì ìˆ˜ ë°ì´í„° í´ë˜ìŠ¤"""
    feature_name: str
    score: float  # 0-100%
    details: Dict
    status: str  # "ìš°ìˆ˜", "ì–‘í˜¸", "ë³´í†µ", "ê°œì„ í•„ìš”"

class PerformanceScorer:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.scores = {}
        
        # ì„±ëŠ¥ ê¸°ì¤€ê°’ ì •ì˜ (ì´ìƒì ì¸ ê°’ë“¤)
        self.benchmarks = {
            "api_response_time_ms": {"excellent": 50, "good": 150, "fair": 300, "poor": 500},
            "concurrent_users_success_rate": {"excellent": 95, "good": 85, "fair": 70, "poor": 50},
            "memory_usage_mb": {"excellent": 100, "good": 250, "fair": 500, "poor": 1000},
            "conversation_flow_time_s": {"excellent": 2.0, "good": 4.0, "fair": 7.0, "poor": 10.0},
            "level_test_time_s": {"excellent": 1.5, "good": 3.0, "fair": 5.0, "poor": 8.0},
            "database_query_ms": {"excellent": 50, "good": 200, "fair": 500, "poor": 1000},
            "cpu_usage_percent": {"excellent": 20, "good": 50, "fair": 70, "poor": 90}
        }
    
    def calculate_performance_score(self, actual_value: float, benchmark_key: str, lower_is_better: bool = True) -> float:
        """ì‹¤ì œ ê°’ê³¼ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë¹„êµí•˜ì—¬ 0-100% ì ìˆ˜ ê³„ì‚°"""
        benchmark = self.benchmarks[benchmark_key]
        
        if lower_is_better:
            # ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ê²½ìš° (ì‘ë‹µì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±)
            if actual_value <= benchmark["excellent"]:
                return 100.0
            elif actual_value <= benchmark["good"]:
                # excellentì™€ good ì‚¬ì´ì˜ ì„ í˜• ë³´ê°„
                ratio = (actual_value - benchmark["excellent"]) / (benchmark["good"] - benchmark["excellent"])
                return 100.0 - (ratio * 15.0)  # 100% -> 85%
            elif actual_value <= benchmark["fair"]:
                ratio = (actual_value - benchmark["good"]) / (benchmark["fair"] - benchmark["good"])
                return 85.0 - (ratio * 15.0)  # 85% -> 70%
            elif actual_value <= benchmark["poor"]:
                ratio = (actual_value - benchmark["fair"]) / (benchmark["poor"] - benchmark["fair"])
                return 70.0 - (ratio * 20.0)  # 70% -> 50%
            else:
                # poor ê¸°ì¤€ë³´ë‹¤ ë‚˜ìœ ê²½ìš°
                excess_ratio = min((actual_value - benchmark["poor"]) / benchmark["poor"], 1.0)
                return max(50.0 - (excess_ratio * 50.0), 0.0)  # 50% -> 0%
        else:
            # ê°’ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ê²½ìš° (ì„±ê³µë¥  ë“±)
            if actual_value >= benchmark["excellent"]:
                return 100.0
            elif actual_value >= benchmark["good"]:
                ratio = (actual_value - benchmark["good"]) / (benchmark["excellent"] - benchmark["good"])
                return 85.0 + (ratio * 15.0)  # 85% -> 100%
            elif actual_value >= benchmark["fair"]:
                ratio = (actual_value - benchmark["fair"]) / (benchmark["good"] - benchmark["fair"])
                return 70.0 + (ratio * 15.0)  # 70% -> 85%
            elif actual_value >= benchmark["poor"]:
                ratio = (actual_value - benchmark["poor"]) / (benchmark["fair"] - benchmark["poor"])
                return 50.0 + (ratio * 20.0)  # 50% -> 70%
            else:
                # poor ê¸°ì¤€ë³´ë‹¤ ë‚˜ìœ ê²½ìš°
                ratio = min(actual_value / benchmark["poor"], 1.0)
                return ratio * 50.0  # 0% -> 50%
    
    def get_status_from_score(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ìƒíƒœ ë¬¸ìì—´ ë°˜í™˜"""
        if score >= 85:
            return "ìš°ìˆ˜"
        elif score >= 70:
            return "ì–‘í˜¸"
        elif score >= 50:
            return "ë³´í†µ"
        else:
            return "ê°œì„ í•„ìš”"
    
    async def test_api_performance(self) -> PerformanceScore:
        """API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (0-100% ì ìˆ˜)"""
        print("ğŸ”Œ API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        endpoints = [
            ("/", "GET"),
            ("/health", "GET"),
            ("/api/situations", "GET"),
            ("/api/languages", "GET")
        ]
        
        all_times = []
        endpoint_details = {}
        
        async with aiohttp.ClientSession() as session:
            for endpoint, method in endpoints:
                times = []
                
                for _ in range(10):
                    start_time = time.time()
                    try:
                        async with session.get(f"{self.base_url}{endpoint}") as resp:
                            await resp.json()
                        end_time = time.time()
                        times.append((end_time - start_time) * 1000)
                    except Exception as e:
                        times.append(5000)  # ì‹¤íŒ¨ì‹œ 5ì´ˆë¡œ ì²˜ë¦¬
                
                avg_time = statistics.mean(times)
                all_times.extend(times)
                endpoint_details[endpoint] = avg_time
        
        overall_avg = statistics.mean(all_times)
        score = self.calculate_performance_score(overall_avg, "api_response_time_ms", lower_is_better=True)
        
        return PerformanceScore(
            feature_name="API ì‘ë‹µì„±",
            score=round(score, 1),
            details={
                "avg_response_time_ms": round(overall_avg, 2),
                "endpoint_details": {k: round(v, 2) for k, v in endpoint_details.items()},
                "benchmark": self.benchmarks["api_response_time_ms"]
            },
            status=self.get_status_from_score(score)
        )
    
    async def test_concurrent_users_performance(self) -> PerformanceScore:
        """ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ‘¥ ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        user_counts = [10, 50, 100]
        success_rates = []
        
        for user_count in user_counts:
            async with aiohttp.ClientSession() as session:
                tasks = []
                
                for i in range(user_count):
                    task = self.simulate_simple_request(session, f"concurrent_test_{i}")
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                successful = sum(1 for r in results if not isinstance(r, Exception))
                success_rate = (successful / user_count) * 100
                success_rates.append(success_rate)
        
        # ê°€ì¥ ê¹Œë‹¤ë¡œìš´ ì¡°ê±´(100ëª…)ì˜ ì„±ê³µë¥ ì„ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
        critical_success_rate = success_rates[-1] if success_rates else 0
        score = self.calculate_performance_score(critical_success_rate, "concurrent_users_success_rate", lower_is_better=False)
        
        return PerformanceScore(
            feature_name="ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬",
            score=round(score, 1),
            details={
                "success_rates_by_users": dict(zip(user_counts, [round(sr, 1) for sr in success_rates])),
                "critical_success_rate": round(critical_success_rate, 1),
                "benchmark": self.benchmarks["concurrent_users_success_rate"]
            },
            status=self.get_status_from_score(score)
        )
    
    async def simulate_simple_request(self, session, user_id):
        """ê°„ë‹¨í•œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜"""
        try:
            async with session.get(f"{self.base_url}/health") as resp:
                if resp.status == 200:
                    return "ì„±ê³µ"
                else:
                    raise Exception(f"HTTP {resp.status}")
        except Exception as e:
            return e
    
    async def test_conversation_flow_performance(self) -> PerformanceScore:
        """ëŒ€í™” í”Œë¡œìš° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ’¬ ëŒ€í™” í”Œë¡œìš° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        flow_times = []
        
        async with aiohttp.ClientSession() as session:
            for i in range(5):  # 5ë²ˆ í…ŒìŠ¤íŠ¸
                start_time = time.time()
                
                try:
                    # ëŒ€í™” ì‹œì‘
                    start_data = {
                        "user_id": f"flow_test_{i}",
                        "situation": "restaurant",
                        "difficulty": "intermediate",
                        "language": "en"
                    }
                    
                    async with session.post(f"{self.base_url}/api/conversation/start", 
                                          json=start_data) as resp:
                        if resp.status == 200:
                            result = await resp.json()
                            session_id = result["data"]["session_id"]
                            
                            # ê°„ë‹¨í•œ ë©”ì‹œì§€ ì „ì†¡
                            message_data = {
                                "session_id": session_id,
                                "message": "Hello",
                                "language": "en"
                            }
                            
                            async with session.post(f"{self.base_url}/api/conversation/text", 
                                                  json=message_data) as msg_resp:
                                await msg_resp.json()
                    
                    end_time = time.time()
                    flow_times.append(end_time - start_time)
                    
                except Exception as e:
                    flow_times.append(10.0)  # ì‹¤íŒ¨ì‹œ 10ì´ˆë¡œ ì²˜ë¦¬
        
        avg_time = statistics.mean(flow_times)
        score = self.calculate_performance_score(avg_time, "conversation_flow_time_s", lower_is_better=True)
        
        return PerformanceScore(
            feature_name="ëŒ€í™” í”Œë¡œìš°",
            score=round(score, 1),
            details={
                "avg_flow_time_s": round(avg_time, 2),
                "min_time_s": round(min(flow_times), 2),
                "max_time_s": round(max(flow_times), 2),
                "benchmark": self.benchmarks["conversation_flow_time_s"]
            },
            status=self.get_status_from_score(score)
        )
    
    async def test_level_test_performance(self) -> PerformanceScore:
        """ë ˆë²¨ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥"""
        print("ğŸ“ ë ˆë²¨ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_times = []
        
        async with aiohttp.ClientSession() as session:
            for i in range(3):  # 3ë²ˆ í…ŒìŠ¤íŠ¸
                start_time = time.time()
                
                try:
                    # ë ˆë²¨ í…ŒìŠ¤íŠ¸ ì‹œì‘
                    start_data = {
                        "user_id": f"level_test_{i}",
                        "language": "english"
                    }
                    
                    async with session.post(f"{self.base_url}/api/level-test/start", 
                                          json=start_data) as resp:
                        if resp.status == 200:
                            result = await resp.json()
                            session_id = result["session_id"]
                            
                            # 3ê°œ ë¬¸ì œ ë‹µë³€
                            for q in range(3):
                                answer_data = {
                                    "session_id": session_id,
                                    "question_id": f"test_q_{q}",
                                    "answer": "A"
                                }
                                
                                async with session.post(f"{self.base_url}/api/level-test/answer", 
                                                      json=answer_data) as answer_resp:
                                    await answer_resp.json()
                    
                    end_time = time.time()
                    test_times.append(end_time - start_time)
                    
                except Exception as e:
                    test_times.append(8.0)  # ì‹¤íŒ¨ì‹œ 8ì´ˆë¡œ ì²˜ë¦¬
        
        avg_time = statistics.mean(test_times) if test_times else 8.0
        score = self.calculate_performance_score(avg_time, "level_test_time_s", lower_is_better=True)
        
        return PerformanceScore(
            feature_name="ë ˆë²¨ í…ŒìŠ¤íŠ¸",
            score=round(score, 1),
            details={
                "avg_test_time_s": round(avg_time, 2),
                "questions_per_second": round(3 / avg_time, 2),
                "benchmark": self.benchmarks["level_test_time_s"]
            },
            status=self.get_status_from_score(score)
        )
    
    def test_system_resources_performance(self) -> PerformanceScore:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ’¾ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # ë©”ëª¨ë¦¬ì™€ CPU ì ìˆ˜ë¥¼ ê°ê° ê³„ì‚° í›„ í‰ê· 
        memory_score = self.calculate_performance_score(memory_mb, "memory_usage_mb", lower_is_better=True)
        cpu_score = self.calculate_performance_score(cpu_percent, "cpu_usage_percent", lower_is_better=True)
        
        overall_score = (memory_score + cpu_score) / 2
        
        return PerformanceScore(
            feature_name="ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤",
            score=round(overall_score, 1),
            details={
                "memory_usage_mb": round(memory_mb, 2),
                "cpu_usage_percent": round(cpu_percent, 1),
                "memory_score": round(memory_score, 1),
                "cpu_score": round(cpu_score, 1),
                "memory_benchmark": self.benchmarks["memory_usage_mb"],
                "cpu_benchmark": self.benchmarks["cpu_usage_percent"]
            },
            status=self.get_status_from_score(overall_score)
        )
    
    async def test_database_performance(self) -> PerformanceScore:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)"""
        print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ì‹¤ì œ DB ì¿¼ë¦¬ ëŒ€ì‹  í†µê³„ API í˜¸ì¶œë¡œ í…ŒìŠ¤íŠ¸
        query_times = []
        
        async with aiohttp.ClientSession() as session:
            for _ in range(5):
                start_time = time.time()
                
                try:
                    async with session.get(f"{self.base_url}/api/data/statistics") as resp:
                        await resp.json()
                    end_time = time.time()
                    query_times.append((end_time - start_time) * 1000)
                except Exception as e:
                    query_times.append(1000)  # ì‹¤íŒ¨ì‹œ 1ì´ˆë¡œ ì²˜ë¦¬
        
        avg_query_time = statistics.mean(query_times)
        score = self.calculate_performance_score(avg_query_time, "database_query_ms", lower_is_better=True)
        
        return PerformanceScore(
            feature_name="ë°ì´í„°ë² ì´ìŠ¤",
            score=round(score, 1),
            details={
                "avg_query_time_ms": round(avg_query_time, 2),
                "min_query_time_ms": round(min(query_times), 2),
                "max_query_time_ms": round(max(query_times), 2),
                "benchmark": self.benchmarks["database_query_ms"]
            },
            status=self.get_status_from_score(score)
        )
    
    async def run_all_performance_tests(self) -> Dict[str, PerformanceScore]:
        """ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ì ìˆ˜ ê³„ì‚°"""
        print("ğŸš€ AI ì–¸ì–´í•™ìŠµ ì•± ì„±ëŠ¥ ì ìˆ˜ ì¸¡ì • ì‹œì‘")
        print("=" * 60)
        
        # ê° ê¸°ëŠ¥ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        tests = [
            self.test_api_performance(),
            self.test_concurrent_users_performance(),
            self.test_conversation_flow_performance(),
            self.test_level_test_performance(),
            self.test_database_performance()
        ]
        
        results = await asyncio.gather(*tests)
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸ (ë™ê¸°)
        system_result = self.test_system_resources_performance()
        results.append(system_result)
        
        # ê²°ê³¼ ì •ë¦¬
        scores = {}
        for result in results:
            scores[result.feature_name] = result
        
        return scores
    
    def generate_score_report(self, scores: Dict[str, PerformanceScore]):
        """ì„±ëŠ¥ ì ìˆ˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ê¸°ëŠ¥ë³„ ì„±ëŠ¥ ì ìˆ˜ ê²°ê³¼")
        print("=" * 60)
        
        total_score = 0
        feature_count = len(scores)
        
        for feature_name, score_data in scores.items():
            score = score_data.score
            status = score_data.status
            total_score += score
            
            # ì ìˆ˜ì— ë”°ë¥¸ ì‹œê°ì  í‘œì‹œ
            bar_length = 20
            filled_length = int(bar_length * score / 100)
            bar = "â–ˆ" * filled_length + "â–‘" * (bar_length - filled_length)
            
            print(f"  {feature_name:<15} â”‚{bar}â”‚ {score:5.1f}% ({status})")
        
        # ì „ì²´ í‰ê·  ì ìˆ˜
        overall_score = total_score / feature_count
        overall_status = self.get_status_from_score(overall_score)
        
        print("â”€" * 60)
        overall_bar_length = 30
        overall_filled = int(overall_bar_length * overall_score / 100)
        overall_bar = "â–ˆ" * overall_filled + "â–‘" * (overall_bar_length - overall_filled)
        
        print(f"  ì „ì²´ í‰ê·  ì ìˆ˜    â”‚{overall_bar}â”‚ {overall_score:5.1f}% ({overall_status})")
        print("=" * 60)
        
        # ìƒì„¸ ì •ë³´ í‘œì‹œ
        print(f"\nğŸ“‹ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„:")
        for feature_name, score_data in scores.items():
            if score_data.score < 70:  # 70% ë¯¸ë§Œì¸ ê¸°ëŠ¥ë“¤ì— ëŒ€í•œ ê°œì„  ì œì•ˆ
                print(f"  âš ï¸  {feature_name}: {score_data.score}% - ê°œì„  ê¶Œì¥")
                
                # ê°œì„  ì œì•ˆ
                if "api" in feature_name.lower():
                    print(f"     ğŸ’¡ ìºì‹±, DB ì¿¼ë¦¬ ìµœì í™”, ì½”ë“œ í”„ë¡œíŒŒì¼ë§ ê¶Œì¥")
                elif "ë™ì‹œ" in feature_name:
                    print(f"     ğŸ’¡ ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”, ì»¤ë„¥ì…˜ í’€ ì¡°ì • ê¶Œì¥")
                elif "ë©”ëª¨ë¦¬" in score_data.details:
                    memory_mb = score_data.details.get("memory_usage_mb", 0)
                    if memory_mb > 500:
                        print(f"     ğŸ’¡ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸, ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™” ê¶Œì¥")
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "overall_score": round(overall_score, 1),
            "overall_status": overall_status,
            "feature_scores": {
                name: {
                    "score": data.score,
                    "status": data.status,
                    "details": data.details
                }
                for name, data in scores.items()
            }
        }
        
        report_file = f"performance_scores_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ìƒì„¸ ì ìˆ˜ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_file}")
        print(f"âœ… ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ! ì „ì²´ ì ìˆ˜: {overall_score:.1f}%")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scorer = PerformanceScorer()
    scores = await scorer.run_all_performance_tests()
    scorer.generate_score_report(scores)

if __name__ == "__main__":
    asyncio.run(main())