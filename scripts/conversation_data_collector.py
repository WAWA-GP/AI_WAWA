"""
ë¬´ë£Œ ì—¬í–‰ ìƒí™©ë³„ ì‹¤ì œ ì–¸ì–´ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ
- ìƒí™©: ê³µí•­, ë ˆìŠ¤í† ë‘, í˜¸í…”, ê¸¸ê±°ë¦¬
- ì–¸ì–´: ì˜ì–´ (ë‚˜ì¤‘ì— ë‹¤ë¥¸ ì–¸ì–´ ì¶”ê°€ ê°€ëŠ¥)
- 100% ì‹¤ì œ ë°ì´í„°ë§Œ ìˆ˜ì§‘ (ì„ì˜ ìƒì„± ê¸ˆì§€)
- ë¬´ë£Œ ì†ŒìŠ¤ë§Œ ì‚¬ìš©: Reddit, êµìœ¡ì‚¬ì´íŠ¸, Cornell Movie Corpus
"""

import asyncio
import aiohttp
import json
import os
import re
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class TravelSituation(Enum):
    AIRPORT = "airport"
    RESTAURANT = "restaurant" 
    HOTEL = "hotel"
    STREET = "street"

class Language(Enum):
    ENGLISH = "en"
    # ë‚˜ì¤‘ì— ì¶”ê°€: KOREAN = "ko", JAPANESE = "ja", etc.

@dataclass
class RealConversationData:
    """ì‹¤ì œ ìˆ˜ì§‘ëœ ëŒ€í™” ë°ì´í„°"""
    situation: TravelSituation
    language: Language
    source: str
    dialogue_text: str
    context: str
    participants: List[str]
    difficulty_level: str
    timestamp: str
    metadata: Dict[str, Any]

class FreeTravelDataCollector:
    """ë¬´ë£Œ ì—¬í–‰ ì–¸ì–´ ë°ì´í„° ìˆ˜ì§‘ê¸° (3ê°œ ì†ŒìŠ¤ë§Œ)"""
    
    def __init__(self, target_language: Language = Language.ENGLISH):
        self.target_language = target_language
        self.collected_data: List[RealConversationData] = []
        
        # ë¬´ë£Œ API í‚¤ë“¤ë§Œ
        self.api_keys = {
            'reddit_client_id': os.getenv('REDDIT_CLIENT_ID'),
            'reddit_client_secret': os.getenv('REDDIT_CLIENT_SECRET'),
        }
        
        # ìƒí™©ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.situation_keywords = {
            TravelSituation.AIRPORT: {
                'primary': ['airport', 'flight', 'gate', 'boarding', 'check-in', 'departure', 'arrival'],
                'secondary': ['passport', 'luggage', 'security', 'terminal', 'customs', 'immigration'],
                'participants': ['passenger', 'agent', 'staff', 'traveler', 'tourist']
            },
            TravelSituation.RESTAURANT: {
                'primary': ['restaurant', 'menu', 'order', 'food', 'waiter', 'server', 'dining'],
                'secondary': ['reservation', 'table', 'bill', 'tip', 'chef', 'kitchen', 'meal'],
                'participants': ['customer', 'guest', 'waiter', 'server', 'host', 'chef']
            },
            TravelSituation.HOTEL: {
                'primary': ['hotel', 'room', 'reception', 'check-in', 'check-out', 'reservation'],
                'secondary': ['lobby', 'key', 'wifi', 'breakfast', 'service', 'concierge'],
                'participants': ['guest', 'receptionist', 'staff', 'concierge', 'housekeeping']
            },
            TravelSituation.STREET: {
                'primary': ['street', 'direction', 'map', 'lost', 'help', 'way', 'location'],
                'secondary': ['bus', 'subway', 'taxi', 'walk', 'turn', 'block', 'intersection'],
                'participants': ['tourist', 'local', 'stranger', 'passerby', 'guide']
            }
        }
    
    # =================================================================
    # ë©”ì¸ ìˆ˜ì§‘ í•¨ìˆ˜ (ë¬´ë£Œ ì†ŒìŠ¤ë§Œ)
    # =================================================================
    
    async def collect_all_travel_data(self) -> List[RealConversationData]:
        """ëª¨ë“  ì—¬í–‰ ìƒí™©ì˜ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ - ë¬´ë£Œ ì†ŒìŠ¤ë§Œ"""
        
        print(f"ğŸŒ ë¬´ë£Œ ì—¬í–‰ ì–¸ì–´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ - ì–¸ì–´: {self.target_language.value}")
        print("ğŸ’° ì‚¬ìš© ì†ŒìŠ¤: Reddit, êµìœ¡ì‚¬ì´íŠ¸, Cornell Movie Corpus (100% ë¬´ë£Œ)")
        print("=" * 60)
        
        # ëª¨ë“  ìƒí™©ì— ëŒ€í•´ ë³‘ë ¬ë¡œ ë°ì´í„° ìˆ˜ì§‘
        collection_tasks = []
        
        for situation in TravelSituation:
            print(f"\nğŸ“ {situation.value.upper()} ìƒí™© ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # ë¬´ë£Œ ì†ŒìŠ¤ 3ê°œë§Œ ì‚¬ìš©
            tasks = [
                self._collect_reddit_real_experiences(situation),
                self._collect_educational_real_content(situation),
                self._collect_movie_subtitle_data(situation)
            ]
            
            collection_tasks.extend(tasks)
        
        # ëª¨ë“  ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
        print("\nğŸ”„ ë¬´ë£Œ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        results = await asyncio.gather(*collection_tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        total_collected = 0
        for result in results:
            if isinstance(result, list):
                self.collected_data.extend(result)
                total_collected += len(result)
            elif isinstance(result, Exception):
                print(f"âš ï¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {result}")
        
        print(f"\nâœ… ì´ {total_collected}ê°œì˜ ì‹¤ì œ ëŒ€í™” ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        # ë°ì´í„° ì •ì œ ë° ê²€ì¦
        validated_data = self._validate_and_clean_data()
        
        return validated_data
    
    # =================================================================
    # Reddit ì‹¤ì œ ê²½í—˜ë‹´ ìˆ˜ì§‘
    # =================================================================
    
    async def _collect_reddit_real_experiences(self, situation: TravelSituation) -> List[RealConversationData]:
        """Redditì—ì„œ ì‹¤ì œ ì—¬í–‰ ê²½í—˜ë‹´ ìˆ˜ì§‘"""
        
        if not (self.api_keys['reddit_client_id'] and self.api_keys['reddit_client_secret']):
            print(f"âš ï¸ Reddit API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤ - {situation.value} ìŠ¤í‚µ")
            return []
        
        # ìƒí™©ë³„ ê´€ë ¨ ì„œë¸Œë ˆë”§
        subreddit_map = {
            TravelSituation.AIRPORT: ['travel', 'solotravel', 'aviation', 'flight'],
            TravelSituation.RESTAURANT: ['travel', 'food', 'solotravel', 'backpacking'],
            TravelSituation.HOTEL: ['travel', 'hotels', 'solotravel', 'hospitality'],
            TravelSituation.STREET: ['travel', 'solotravel', 'backpacking', 'AskReddit']
        }
        
        real_experiences = []
        
        try:
            import praw
            
            reddit = praw.Reddit(
                client_id=self.api_keys['reddit_client_id'],
                client_secret=self.api_keys['reddit_client_secret'],
                user_agent='TravelLanguageLearning/1.0'
            )
            
            for subreddit_name in subreddit_map[situation]:
                subreddit = reddit.subreddit(subreddit_name)
                
                # ìƒí™© ê´€ë ¨ ê²€ìƒ‰ì–´
                search_terms = self.situation_keywords[situation]['primary'][:3]
                
                for term in search_terms:
                    try:
                        for submission in subreddit.search(f"{term} conversation", limit=5):
                            # ê²Œì‹œë¬¼ ë³¸ë¬¸ì—ì„œ ì‹¤ì œ ëŒ€í™” ì¶”ì¶œ
                            if submission.selftext and len(submission.selftext) > 100:
                                dialogue_data = self._extract_reddit_dialogue(
                                    submission.title,
                                    submission.selftext,
                                    situation,
                                    subreddit_name
                                )
                                if dialogue_data:
                                    real_experiences.append(dialogue_data)
                            
                            # ëŒ“ê¸€ì—ì„œë„ ì‹¤ì œ ê²½í—˜ ìˆ˜ì§‘
                            submission.comments.replace_more(limit=0)
                            for comment in submission.comments.list()[:10]:
                                if self._is_real_travel_experience(comment.body, situation):
                                    comment_data = RealConversationData(
                                        situation=situation,
                                        language=self.target_language,
                                        source=f"reddit_{subreddit_name}_comment",
                                        dialogue_text=comment.body,
                                        context=f"Reddit comment on: {submission.title[:50]}...",
                                        participants=self._extract_participants(comment.body, situation),
                                        difficulty_level=self._assess_difficulty_level(comment.body),
                                        timestamp=datetime.now().isoformat(),
                                        metadata={
                                            'subreddit': subreddit_name,
                                            'post_title': submission.title
                                        }
                                    )
                                    real_experiences.append(comment_data)
                    
                    except Exception as e:
                        print(f"Reddit ê²€ìƒ‰ ì˜¤ë¥˜ ({term}): {e}")
                        continue
        
        except ImportError:
            print("âš ï¸ PRAW ì„¤ì¹˜ í•„ìš”: pip install praw")
            return []
        except Exception as e:
            print(f"âŒ Reddit {situation.value} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        print(f"ğŸ’¬ Reddit {situation.value}: {len(real_experiences)}ê°œ ìˆ˜ì§‘")
        return real_experiences
    
    def _extract_reddit_dialogue(self, title: str, content: str, situation: TravelSituation, subreddit: str) -> Optional[RealConversationData]:
        """Reddit ê²Œì‹œë¬¼ì—ì„œ ì‹¤ì œ ëŒ€í™” ì¶”ì¶œ"""
        
        # ì‹¤ì œ ëŒ€í™”ê°€ í¬í•¨ëœ ê²½ìš°ë§Œ
        dialogue_indicators = ['"', "'", 'said', 'told', 'asked', 'replied', 'he said', 'she said']
        
        if any(indicator in content for indicator in dialogue_indicators):
            # ìƒí™© ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
            situation_keywords = self.situation_keywords[situation]['primary']
            if any(keyword in content.lower() for keyword in situation_keywords):
                
                return RealConversationData(
                    situation=situation,
                    language=self.target_language,
                    source=f"reddit_{subreddit}_post",
                    dialogue_text=content,
                    context=f"Reddit post: {title}",
                    participants=self._extract_participants(content, situation),
                    difficulty_level=self._assess_difficulty_level(content),
                    timestamp=datetime.now().isoformat(),
                    metadata={
                        'subreddit': subreddit,
                        'post_title': title
                    }
                )
        
        return None
    
    # =================================================================
    # êµìœ¡ ìë£Œ ì‹¤ì œ ëŒ€í™” ìˆ˜ì§‘
    # =================================================================
    
    async def _collect_educational_real_content(self, situation: TravelSituation) -> List[RealConversationData]:
        """êµìœ¡ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ëŒ€í™” ì˜ˆì‹œ ìˆ˜ì§‘"""
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ì–´ êµìœ¡ ì‚¬ì´íŠ¸ë“¤
        educational_urls = {
            TravelSituation.AIRPORT: [
                'https://www.englishclub.com/english-for-work/airport-english.htm',
                'https://www.fluentu.com/blog/english/english-airport-conversation/'
            ],
            TravelSituation.RESTAURANT: [
                'https://www.englishclub.com/english-for-work/restaurant-english.htm',
                'https://www.fluentu.com/blog/english/restaurant-english/'
            ],
            TravelSituation.HOTEL: [
                'https://www.englishclub.com/english-for-work/hotel-english.htm',
                'https://www.fluentu.com/blog/english/hotel-english/'
            ],
            TravelSituation.STREET: [
                'https://www.englishclub.com/vocabulary/directions.htm',
                'https://www.fluentu.com/blog/english/asking-for-directions-english/'
            ]
        }
        
        real_educational_content = []
        
        try:
            from bs4 import BeautifulSoup
            
            async with aiohttp.ClientSession() as session:
                for url in educational_urls.get(situation, []):
                    try:
                        async with session.get(url) as response:
                            if response.status == 200:
                                html = await response.text()
                                soup = BeautifulSoup(html, 'html.parser')
                                
                                # ëŒ€í™” ì˜ˆì‹œ ì¶”ì¶œ
                                dialogue_elements = soup.find_all(['div', 'p'], class_=lambda x: x and ('dialogue' in x.lower() or 'conversation' in x.lower()))
                                
                                for element in dialogue_elements:
                                    text = element.get_text().strip()
                                    if self._is_structured_dialogue(text):
                                        dialogue_data = RealConversationData(
                                            situation=situation,
                                            language=self.target_language,
                                            source=f"educational_{url.split('//')[1].split('/')[0]}",
                                            dialogue_text=text,
                                            context=f"Educational content from {url}",
                                            participants=self._extract_participants(text, situation),
                                            difficulty_level="beginner",  # êµìœ¡ ìë£ŒëŠ” ë³´í†µ ì´ˆê¸‰
                                            timestamp=datetime.now().isoformat(),
                                            metadata={
                                                'source_url': url,
                                                'content_type': 'educational_example'
                                            }
                                        )
                                        real_educational_content.append(dialogue_data)
                        
                        await asyncio.sleep(2)  # ì›¹ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€
                        
                    except Exception as e:
                        print(f"êµìœ¡ ì‚¬ì´íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜ ({url}): {e}")
                        continue
        
        except ImportError:
            print("âš ï¸ BeautifulSoup ì„¤ì¹˜ í•„ìš”: pip install beautifulsoup4")
        
        print(f"ğŸ“š Educational {situation.value}: {len(real_educational_content)}ê°œ ìˆ˜ì§‘")
        return real_educational_content
    
    # =================================================================
    # ì˜í™” ìë§‰ ì‹¤ì œ ëŒ€í™” ìˆ˜ì§‘
    # =================================================================
    
    async def _collect_movie_subtitle_data(self, situation: TravelSituation) -> List[RealConversationData]:
        """ì˜í™” ìë§‰ì—ì„œ ìƒí™©ë³„ ì‹¤ì œ ëŒ€í™” ìˆ˜ì§‘"""
        
        # Cornell Movie Dialogs Corpus ì‚¬ìš©
        movie_dialogues = await self._collect_cornell_movie_dialogues(situation)
        
        print(f"ğŸ¬ Movies {situation.value}: {len(movie_dialogues)}ê°œ ìˆ˜ì§‘")
        return movie_dialogues
    
    async def _collect_cornell_movie_dialogues(self, situation: TravelSituation) -> List[RealConversationData]:
        """Cornell Movie Dialogsì—ì„œ ìƒí™©ë³„ ëŒ€í™” ì¶”ì¶œ"""
        
        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ Cornell ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        cornell_file = 'cornell_data/cornell movie-dialogs corpus/movie_lines.txt'
        
        if not os.path.exists(cornell_file):
            print(f"âš ï¸ Cornell Movie Dialogs ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ - {situation.value} ìŠ¤í‚µ")
            print(f"ğŸ’¡ ë‹¤ìš´ë¡œë“œ: http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html")
            return []
        
        movie_dialogues = []
        situation_keywords = self.situation_keywords[situation]['primary']
        
        try:
            with open(cornell_file, 'r', encoding='iso-8859-1') as f:
                lines = f.readlines()
                
                for line in lines:
                    parts = line.split(' +++$+++ ')
                    if len(parts) >= 5:
                        dialogue_text = parts[4].strip()
                        
                        # ìƒí™© ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                        if any(keyword in dialogue_text.lower() for keyword in situation_keywords):
                            movie_data = RealConversationData(
                                situation=situation,
                                language=self.target_language,
                                source=f"cornell_movie_corpus",
                                dialogue_text=dialogue_text,
                                context=f"Movie dialogue from Cornell corpus",
                                participants=['character1', 'character2'],
                                difficulty_level=self._assess_difficulty_level(dialogue_text),
                                timestamp=datetime.now().isoformat(),
                                metadata={
                                    'movie_id': parts[2] if len(parts) > 2 else 'unknown',
                                    'character_id': parts[1] if len(parts) > 1 else 'unknown'
                                }
                            )
                            movie_dialogues.append(movie_data)
        
        except Exception as e:
            print(f"Cornell ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return movie_dialogues
    
    # =================================================================
    # ë°ì´í„° ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    # =================================================================
    
    def _is_real_travel_experience(self, text: str, situation: TravelSituation) -> bool:
        """ì‹¤ì œ ì—¬í–‰ ê²½í—˜ë‹´ì¸ì§€ í™•ì¸"""
        experience_indicators = ['i was', 'when i', 'at the', 'the staff', 'they said', 'i asked']
        situation_keywords = self.situation_keywords[situation]['primary']
        
        return (len(text) > 50 and 
                any(indicator in text.lower() for indicator in experience_indicators) and
                any(keyword in text.lower() for keyword in situation_keywords))
    
    def _is_structured_dialogue(self, text: str) -> bool:
        """êµ¬ì¡°í™”ëœ ëŒ€í™”ì¸ì§€ í™•ì¸"""
        dialogue_indicators = [':', 'A:', 'B:', 'Customer:', 'Staff:', 'Tourist:', 'Local:']
        return any(indicator in text for indicator in dialogue_indicators)
    
    def _extract_participants(self, text: str, situation: TravelSituation) -> List[str]:
        """ëŒ€í™” ì°¸ì—¬ì ì¶”ì¶œ"""
        participants = self.situation_keywords[situation]['participants']
        found_participants = []
        
        for participant in participants:
            if participant in text.lower():
                found_participants.append(participant)
        
        return found_participants if found_participants else ['person1', 'person2']
    
    def _assess_difficulty_level(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ë‚œì´ë„ í‰ê°€"""
        word_count = len(text.split())
        avg_word_length = sum(len(word) for word in text.split()) / word_count if word_count > 0 else 0
        
        if word_count < 30 and avg_word_length < 5:
            return "beginner"
        elif word_count < 80 and avg_word_length < 6:
            return "intermediate"
        else:
            return "advanced"
    
    def _validate_and_clean_data(self) -> List[RealConversationData]:
        """ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
        
        validated_data = []
        
        for data in self.collected_data:
            # ìµœì†Œ ê¸¸ì´ í™•ì¸
            if len(data.dialogue_text.strip()) < 20:
                continue
            
            # ì¤‘ë³µ ì œê±°
            if any(existing.dialogue_text == data.dialogue_text for existing in validated_data):
                continue
            
            # ì–¸ì–´ í™•ì¸ (ê°„ë‹¨í•œ ë°©ë²•)
            if self._is_target_language(data.dialogue_text):
                validated_data.append(data)
        
        print(f"ğŸ” ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(validated_data)}ê°œ ìœ íš¨í•œ ë°ì´í„°")
        return validated_data
    
    def _is_target_language(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ëª©í‘œ ì–¸ì–´ì¸ì§€ í™•ì¸"""
        # ê°„ë‹¨í•œ ì˜ì–´ í™•ì¸ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì–¸ì–´ ê°ì§€ í•„ìš”)
        english_indicators = ['the', 'and', 'is', 'are', 'was', 'were', 'have', 'has']
        return any(indicator in text.lower() for indicator in english_indicators)
    
    # =================================================================
    # ë°ì´í„° ì €ì¥ ë° ë‚´ë³´ë‚´ê¸°
    # =================================================================
    
    def save_collected_data(self, filename: Optional[str] = None) -> str:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"free_travel_conversations_{self.target_language.value}_{timestamp}.json"
        
        # ë°ì´í„°ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        export_data = {
            'metadata': {
                'collection_date': datetime.now().isoformat(),
                'target_language': self.target_language.value,
                'total_conversations': len(self.collected_data),
                'sources_used': ['reddit', 'educational_websites', 'cornell_movie_corpus'],
                'cost': 'FREE',
                'situations': list(set(data.situation.value for data in self.collected_data)),
                'sources': list(set(data.source for data in self.collected_data))
            },
            'conversations': []
        }
        
        # ìƒí™©ë³„ë¡œ ë°ì´í„° ì •ë¦¬
        situation_stats = {}
        for data in self.collected_data:
            situation = data.situation.value
            if situation not in situation_stats:
                situation_stats[situation] = 0
            situation_stats[situation] += 1
            
            conversation_entry = {
                'id': len(export_data['conversations']) + 1,
                'situation': data.situation.value,
                'language': data.language.value,
                'source': data.source,
                'dialogue': data.dialogue_text,
                'context': data.context,
                'participants': data.participants,
                'difficulty_level': data.difficulty_level,
                'timestamp': data.timestamp,
                'metadata': data.metadata
            }
            export_data['conversations'].append(conversation_entry)
        
        export_data['metadata']['situation_breakdown'] = situation_stats
        
        # íŒŒì¼ ì €ì¥
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"ğŸ“Š ìƒí™©ë³„ ë°ì´í„° ìˆ˜:")
        for situation, count in situation_stats.items():
            print(f"   - {situation}: {count}ê°œ")
        
        return filename
    
    def export_for_ai_training(self, output_dir: str = "training_data") -> Dict[str, str]:
        """AI í•™ìŠµìš© í¬ë§·ìœ¼ë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        exported_files = {}
        
        # ìƒí™©ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
        for situation in TravelSituation:
            situation_data = [
                data for data in self.collected_data 
                if data.situation == situation
            ]
            
            if not situation_data:
                continue
            
            # AI í•™ìŠµìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜
            training_format = {
                'dataset_info': {
                    'situation': situation.value,
                    'language': self.target_language.value,
                    'total_samples': len(situation_data),
                    'purpose': 'travel_conversation_training',
                    'cost': 'FREE'
                },
                'training_data': []
            }
            
            for data in situation_data:
                # ëŒ€í™”ë¥¼ ì…ë ¥-ì¶œë ¥ ìŒìœ¼ë¡œ ë³€í™˜
                dialogue_lines = data.dialogue_text.split('\n')
                
                for i in range(0, len(dialogue_lines) - 1, 2):
                    if i + 1 < len(dialogue_lines):
                        training_sample = {
                            'input': dialogue_lines[i].strip(),
                            'output': dialogue_lines[i + 1].strip(),
                            'context': {
                                'situation': situation.value,
                                'participants': data.participants,
                                'difficulty': data.difficulty_level,
                                'source': data.source
                            }
                        }
                        training_format['training_data'].append(training_sample)
            
            # íŒŒì¼ ì €ì¥
            filename = f"{output_dir}/{situation.value}_free_training_{self.target_language.value}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(training_format, f, indent=2, ensure_ascii=False)
            
            exported_files[situation.value] = filename
            print(f"ğŸ“ {situation.value} í•™ìŠµ ë°ì´í„°: {filename}")
        
        return exported_files
    
    def generate_statistics_report(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í†µê³„ ë³´ê³ ì„œ ìƒì„±"""
        
        if not self.collected_data:
            return {"error": "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ê¸°ë³¸ í†µê³„
        total_conversations = len(self.collected_data)
        
        # ìƒí™©ë³„ í†µê³„
        situation_stats = {}
        for situation in TravelSituation:
            count = len([d for d in self.collected_data if d.situation == situation])
            situation_stats[situation.value] = count
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        source_stats = {}
        for data in self.collected_data:
            source_type = data.source.split('_')[0]  # ì†ŒìŠ¤ íƒ€ì… ì¶”ì¶œ
            source_stats[source_type] = source_stats.get(source_type, 0) + 1
        
        # ë‚œì´ë„ë³„ í†µê³„
        difficulty_stats = {}
        for data in self.collected_data:
            difficulty_stats[data.difficulty_level] = difficulty_stats.get(data.difficulty_level, 0) + 1
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
        text_lengths = [len(data.dialogue_text.split()) for data in self.collected_data]
        avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
        
        report = {
            'collection_summary': {
                'total_conversations': total_conversations,
                'target_language': self.target_language.value,
                'collection_date': datetime.now().isoformat(),
                'average_text_length': round(avg_length, 1),
                'cost': 'FREE'
            },
            'situation_breakdown': situation_stats,
            'source_breakdown': source_stats,
            'difficulty_breakdown': difficulty_stats,
            'quality_metrics': {
                'min_length': min(text_lengths) if text_lengths else 0,
                'max_length': max(text_lengths) if text_lengths else 0,
                'median_length': sorted(text_lengths)[len(text_lengths)//2] if text_lengths else 0
            }
        }
        
        return report


# =================================================================
# ì‚¬ìš© ì˜ˆì‹œ ë° ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =================================================================

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ë¬´ë£Œ ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •"""
    
    print("ğŸŒ ë¬´ë£Œ ì—¬í–‰ ë§ì¶¤ ì–¸ì–´ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ê¸°")
    print("=" * 60)
    print("ìƒí™©: ê³µí•­, ë ˆìŠ¤í† ë‘, í˜¸í…”, ê¸¸ê±°ë¦¬")
    print("ì–¸ì–´: ì˜ì–´ (ë‚˜ì¤‘ì— ë‹¤ë¥¸ ì–¸ì–´ ì¶”ê°€ ê°€ëŠ¥)")
    print("ì›ì¹™: 100% ì‹¤ì œ ë°ì´í„°ë§Œ ìˆ˜ì§‘")
    print("ğŸ’° ë¹„ìš©: ë¬´ë£Œ (Reddit + êµìœ¡ì‚¬ì´íŠ¸ + Cornell)")
    print("=" * 60)
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = FreeTravelDataCollector(Language.ENGLISH)
    
    # ëª¨ë“  ìƒí™©ì˜ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
    collected_data = await collector.collect_all_travel_data()
    
    # ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
    if collected_data:
        print(f"\nğŸ‰ ë¬´ë£Œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        
        # í†µê³„ ë³´ê³ ì„œ ìƒì„±
        stats = collector.generate_statistics_report()
        print(f"\nğŸ“Š ìˆ˜ì§‘ í†µê³„:")
        print(f"   ì´ ëŒ€í™”: {stats['collection_summary']['total_conversations']}ê°œ")
        print(f"   í‰ê·  ê¸¸ì´: {stats['collection_summary']['average_text_length']} ë‹¨ì–´")
        print(f"   ğŸ’° ë¹„ìš©: {stats['collection_summary']['cost']}")
        
        print(f"\nğŸ“ ìƒí™©ë³„ ë°ì´í„°:")
        for situation, count in stats['situation_breakdown'].items():
            print(f"   - {situation}: {count}ê°œ")
        
        print(f"\nğŸ“š ì†ŒìŠ¤ë³„ ë°ì´í„°:")
        for source, count in stats['source_breakdown'].items():
            print(f"   - {source}: {count}ê°œ")
        
        # ë°ì´í„° ì €ì¥
        saved_file = collector.save_collected_data()
        
        # AI í•™ìŠµìš© ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        print(f"\nğŸ¤– AI í•™ìŠµìš© ë°ì´í„° ë‚´ë³´ë‚´ê¸°...")
        training_files = collector.export_for_ai_training()
        
        print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print(f"ğŸ“ ì›ë³¸ ë°ì´í„°: {saved_file}")
        print(f"ğŸ“ í•™ìŠµ ë°ì´í„°: {len(training_files)}ê°œ íŒŒì¼ ìƒì„±")
        
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

def setup_free_api_keys():
    """ë¬´ë£Œ API í‚¤ ì„¤ì • ê°€ì´ë“œ"""
    
    required_keys = {
        'REDDIT_CLIENT_ID': 'https://www.reddit.com/prefs/apps',
        'REDDIT_CLIENT_SECRET': 'https://www.reddit.com/prefs/apps',
    }
    
    print("ğŸ”‘ í•„ìš”í•œ ë¬´ë£Œ API í‚¤ ì„¤ì •:")
    print("=" * 40)
    
    for key, url in required_keys.items():
        value = os.getenv(key)
        status = "âœ… ì„¤ì •ë¨" if value else "âŒ ë¯¸ì„¤ì •"
        print(f"{key}: {status}")
        if not value:
            print(f"   ë°œê¸‰ URL: {url}")
    
    print("\nğŸ’¡ .env íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
    print("REDDIT_CLIENT_ID=your_reddit_id_here")
    print("REDDIT_CLIENT_SECRET=your_reddit_secret_here")
    
    print("\nğŸ“‹ ë¬´ë£Œ ë°ì´í„° ì†ŒìŠ¤:")
    print("1. Reddit API - ë¬´ë£Œ (ì‹¤ì œ ì—¬í–‰ ê²½í—˜ë‹´)")
    print("2. êµìœ¡ ì›¹ì‚¬ì´íŠ¸ - ë¬´ë£Œ (ì˜ì–´ í•™ìŠµ ëŒ€í™” ì˜ˆì‹œ)")
    print("3. Cornell Movie Corpus - ë¬´ë£Œ (ì˜í™” ëŒ€í™” ë°ì´í„°)")

def download_cornell_data():
    """Cornell Movie Corpus ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ"""
    
    print("\nğŸ¬ Cornell Movie Corpus ë‹¤ìš´ë¡œë“œ:")
    print("=" * 40)
    print("1. ì›¹ì‚¬ì´íŠ¸: http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html")
    print("2. ë‹¤ìš´ë¡œë“œ: cornell movie-dialogs corpus.zip")
    print("3. ì••ì¶• í•´ì œ: cornell_data/ í´ë”ì—")
    print("4. íŒŒì¼ ìœ„ì¹˜: cornell_data/cornell movie-dialogs corpus/movie_lines.txt")
    print("\nğŸ’¡ ì„ íƒì‚¬í•­: Cornell ë°ì´í„° ì—†ì´ë„ Reddit + êµìœ¡ì‚¬ì´íŠ¸ë¡œ ìˆ˜ì§‘ ê°€ëŠ¥")

if __name__ == "__main__":
    print("ğŸš€ ë¬´ë£Œ ì—¬í–‰ ì–¸ì–´ ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘!")
    
    # ë¬´ë£Œ API í‚¤ í™•ì¸
    setup_free_api_keys()
    
    # Cornell ë°ì´í„° ê°€ì´ë“œ
    download_cornell_data()
    
    # ì‚¬ìš©ì í™•ì¸
    proceed = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
    
    if proceed.lower() == 'y':
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸
        try:
            import aiohttp
            import praw
            from bs4 import BeautifulSoup
            
            # ë©”ì¸ ìˆ˜ì§‘ ì‹¤í–‰
            asyncio.run(main())
            
        except ImportError as e:
            print(f"\nâŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
            print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("pip install aiohttp praw beautifulsoup4")
    else:
        print("ğŸ‘‹ ë°ì´í„° ìˆ˜ì§‘ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")